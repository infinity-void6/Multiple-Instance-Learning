Folder Structure
--------------------------------------------------
Code/
    metadata_dataset.py
    extract_features.py
    feature_extraction_dataset.py
    extract_segments.py
    device check.py
    unprocessed_video_data_extraction.py
    Feature_Dataset_Class.py
    debugging.ipynb
    .gitignore
    loss_function.py
    split_data_train_val_test.py
    epochs_training_testing.py
    similarity_scores.txt
    trained_model.pth
    unused_code.py
    find_max_segments.py
    Sequential_Model.py
    __pycache__/
        segmentation.cpython-312.pyc
        video_feature_extraction.cpython-312.pyc
        Feature_Dataset_Class.cpython-312.pyc
        extract_segments.cpython-312.pyc
        extract_features.cpython-312.pyc
        split_data_train_val_test.cpython-312.pyc
        loss_function.cpython-312.pyc
        Sequential_Model.cpython-312.pyc
    debug_processed_features/
        Normal_Videos_924_x264_features.pt
        Explosion002_x264_features.pt
    processed_features/

    .git/
        description
        config
        HEAD
        hooks/
            applypatch-msg.sample
            commit-msg.sample
            fsmonitor-watchman.sample
            post-update.sample
            pre-applypatch.sample
            pre-commit.sample
            pre-merge-commit.sample
            pre-push.sample
            pre-rebase.sample
            pre-receive.sample
            prepare-commit-msg.sample
            push-to-checkout.sample
            sendemail-validate.sample
            update.sample
        info/
            exclude
        refs/
            heads/
            tags/
        objects/
            pack/
            info/
    videos/
        srm_sambavam.mp4
        normal_video.mp4
        truck_accident.mp4
        ironman.mp4
        scenery.mp4
        walking.mp4
        CCTV Captures Massive Car vs Truck Crash at Intersection in Mbombela.mp4
    .kaggle/
        kaggle.json
    Normal/
        real-time-anomaly-detection-in-cctv-surveillance.zip
    normal_features/
     anomaly_features/
     split/
        train/
            normal/
            anomalous/
        val/
            normal/
            anomalous/
        test/
            normal/
            anomalous/
                

File Contents
--------------------------------------------------


E:\MIL\Code\metadata_dataset.py
File type: .py
import os
import pandas as pd

def create_weakly_supervised_metadata(normal_dir, anomaly_dir, output_csv):
    """
    Create metadata for weakly supervised learning with binary labels (0 for anomalous, 1 for normal).
    
    Args:
        normal_dir (str): Path to the directory containing normal videos.
        anomaly_dir (str): Path to the directory containing anomalous videos (subdirectories for each class).
        output_csv (str): Path to save the metadata CSV file.
    """
    data = []

    # Process normal videos (Label 1)
    for video in os.listdir(normal_dir):
        if video.endswith(('.mp4', '.avi', '.mkv')):  # Include only video files
            data.append({
                "file_path": os.path.join(normal_dir, video),
                "label": 1
            })

    # Process anomalous videos (Label 0)
    for root, _, files in os.walk(anomaly_dir):
        for video in files:
            if video.endswith(('.mp4', '.avi', '.mkv')):  # Include only video files
                data.append({
                    "file_path": os.path.join(root, video),
                    "label": 0  # Label for anomalous videos
                })

    # Save metadata to CSV
    df = pd.DataFrame(data)
    print(df)
    print(len(df))
    df.to_csv(output_csv, index=False)
    print(f"Metadata saved to {output_csv}")

# Example usage
normal_dir = r"E:\MIL\Dataset\Normal-Videos"
anomaly_dir = r"E:\MIL\Dataset\Anomaly-Videos"
output_csv = r"E:\MIL\metadata.csv"

create_weakly_supervised_metadata(normal_dir, anomaly_dir, output_csv)



--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\extract_features.py
File type: .py
# Feature Extractor
from pytorchvideo.models.hub import x3d_s
import torch
from torch import nn
from extract_segments import extract_segments_from_video

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pretrained x3d_s model
model = x3d_s(pretrained=True).to(device)

# Modify block 5 to include explicit global average pooling
class FeatureExtractor(torch.nn.Module):
    def __init__(self, model):
        super(FeatureExtractor, self).__init__()
        self.backbone = torch.nn.Sequential(*model.blocks[:-1])  # Blocks 0-4
        self.final_block = model.blocks[5].pool  # Block 5 (unchanged)
        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))  # Define global average pooling layer

    def forward(self, x):
        #print(f"Input shape: {x.shape}")  # Debug input shape
        x = self.backbone(x)  # Pass through blocks 0-4
        #print(f"After backbone shape: {x.shape}")  # Debug backbone output shape
        x=self.final_block(x)
        #print(f"After final_block shape: {x.shape}")  # Debug backbone output shape
        x = self.pool(x)  # Pass through block 5 pooling
        #print(f"After pooling shape: {x.shape}")  # Debug pooling output shape

        x = torch.mean(x, dim=[-3, -2, -1], keepdim=True)  # Apply global average pooling
        #print(f"After global average pooling shape: {x.shape}")  # Debug GAP output shape

        return x

# Initialize the feature extractor
feature_extractor = FeatureExtractor(model)

# Define feature extraction function
def extract_features(segments, device, model=feature_extractor):
    """
    Extract features from video segments using the modified model.

    Args:
        model (torch.nn.Module): Pretrained and modified model.
        segments (list): List of video segments (tensors).
        device (torch.device): Device for computation.

    Returns:
        list: List of feature tensors.
    """
    features = []
    model.eval()
    with torch.no_grad():
        for segment in segments:
            segment = segment.unsqueeze(0).to(device)  # Add batch dimension and move to device
            feature = model(segment)  # Extract features
            features.append(feature.squeeze(0).cpu())  # Remove batch dimension and move to CPU
    return features

if __name__ == "__main__":
    file = r"E:\MIL\Dataset\Anomaly-Videos\Anomaly-Videos-Part-2\Explosion\Explosion001_x264.mp4"
    list_segment = extract_segments_from_video(file, segment_size=16, target_shape=(240, 320), frame_skip=2)
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    features=extract_features([list_segment[0]],device)
    print(features)
    for idx, segment in enumerate(list_segment):
        print(f"Segment {idx + 1} shape: {segment.shape}")
        break  # Only print the first segment for testing
'''
FIRST MODEL 
________________
from pytorchvideo.models.hub import x3d_s
import torch

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
model=x3d_s(pretrained=True).to(device)
model.blocks[5]=torch.nn.Identity()

def extract_features(segments,device,model=model):
    """
    Extract features from video segments using a pretrained model.

    Args:
        model (torch.nn.Module): Pretrained model.
        segments (list): List of video segments (tensors).
        device (torch.device): Device for computation.

    Returns:
        list: List of feature tensors.
    """
    features=[]
    model.eval()
    with torch.no_grad():
        for segment in segments:
            segment=segment.unsqueeze(0).to(device)
            feature=model(segment)
            features.append(feature.squeeze(0).cpu())
    return features
'''

--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\feature_extraction_dataset.py
File type: .py
# Import necessary libraries
from extract_segments import extract_segments_from_video  # Function to divide videos into segments
from extract_features import extract_features  # Function to extract features from segments
import torch  # PyTorch for tensor operations and GPU acceleration
import os  # For file and directory management
import pandas as pd  # For handling metadata
from tqdm import tqdm  # For progress bar
from torch.amp import autocast  # For mixed-precision acceleration
import cv2  # OpenCV for video processing

# Define file paths and output directories
file_path = r"E:\MIL\metadata.csv"  # Path to the metadata CSV file
normal_output_dir = 'normal_features'  # Directory to save normal video features
anomaly_output_dir = 'anomaly_features'  # Directory to save anomaly video features

# Create the directories if they don't exist
os.makedirs(normal_output_dir, exist_ok=True)
os.makedirs(anomaly_output_dir, exist_ok=True)

# Check if the metadata file exists
if not os.path.exists(file_path):
    raise FileNotFoundError(f'Metadata file not found at {file_path}')  # Raise an error if the file is missing

# Load metadata into a Pandas DataFrame
metadata = pd.read_csv(file_path)

# Check if CUDA (GPU) is available and set the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise fallback to CPU

# Define batch size for segment processing
batch_size = 4  # Start small to avoid out-of-memory (OOM) errors

# Function to determine frame_skip based on video length
# def determine_frame_skip(video_path):
#     cap = cv2.VideoCapture(video_path)  # Open video file
#     fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second
#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames
#     duration = frame_count / fps  # Calculate video duration in seconds
#     cap.release()  # Release the video file

#     if duration <= 300:  # Video length <= 5 minutes
#         return 2
#     elif duration <= 600:  # Video length <= 10 minutes
#         return 5
#     else:  # Video length > 10 minutes
#         return 10

def determine_frame_skip(video_path):
    cap = cv2.VideoCapture(video_path)  # Open video file
    fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames
    duration = frame_count / fps  # Calculate video duration in seconds
    cap.release()  # Release the video file

    if duration <= 300:  # Video length <= 5 minutes
        return 2
    elif duration <= 600:  # Video length <= 10 minutes
        return 5
    elif duration <= 1800:  # Video length <= 30 minutes
        return 10
    elif duration <= 3600:  # Video length <= 1 hour
        return 30
    else:  # Video length > 1 hour
        return 60



# Loop through each video in the metadata file
for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc="Processing Videos"):
    video_path = row['file_path']  # Path to the video file
    label = row['label']  # Label associated with the video (e.g., 0 for normal, 1 for anomaly)

    try:
        # Check if the video file exists
        if not os.path.exists(video_path):
            tqdm.write(f"Warning: Video file not found: {video_path}")
            continue

        # Determine frame_skip based on video length
        frame_skip = determine_frame_skip(video_path)
        tqdm.write(f"Using frame_skip={frame_skip} for video: {video_path}")

        # Step 1: Extract segments from the video with frame skipping
        segments = extract_segments_from_video(video_path, frame_skip=frame_skip)  # Pass frame_skip to function

        # Step 2: Ensure all segments have consistent temporal size
        processed_segments = []
        for segment in segments:
            if segment.size(1) < 16:  # If the segment has fewer than 16 frames
                pad_size = 16 - segment.size(1)  # Calculate the number of frames to pad
                padding = torch.zeros((segment.size(0), pad_size, segment.size(2), segment.size(3)))  # Create padding
                segment = torch.cat([segment, padding], dim=1)  # Add padding to the temporal dimension
            elif segment.size(1) > 16:  # If the segment has more than 16 frames
                segment = segment[:, :16, :, :]  # Truncate to the first 16 frames
            processed_segments.append(segment)
        segments = processed_segments  # Replace original segments with the processed ones

        # Step 3: Divide segments into smaller batches
        batches = [segments[i:i + batch_size] for i in range(0, len(segments), batch_size)]

        # Step 4: Extract features for each batch
        features = []
        for batch in batches:
            batch = torch.stack(batch).to(device)  # Stack batch into a tensor and move to GPU
            with autocast("cuda"):  # Use mixed precision to save memory and accelerate processing
                batch_features = extract_features(batch, device)  # Extract features for the batch

            # Ensure batch_features is a tensor
            if isinstance(batch_features, list):  
                batch_features = torch.stack(batch_features)  # Convert list to tensor if needed

            features.append(batch_features.cpu())  # Move features to CPU to free up GPU memory

        # Combine features from all batches into a single tensor
        features = torch.cat(features, dim=0)

        # Step 5: Save the features and label to a .pt file
        video_name = os.path.splitext(os.path.basename(video_path))[0]  # Extract the video name (without extension)
        if label == 1:
            output_path = os.path.join(normal_output_dir, f"{video_name}_features.pt")
        else:
            output_path = os.path.join(anomaly_output_dir, f"{video_name}_features.pt")

        torch.save({
            "features": features,  # Tensor containing extracted features
            "label": label  # Label associated with the video
        }, output_path)  # Save to the appropriate directory

        # Clear unused GPU memory
        torch.cuda.empty_cache()

        # Log the successful processing of the video
        tqdm.write(f"Processed video: {video_name}, Label: {label}")

    # Handle out-of-memory errors
    except RuntimeError as e:
        if "out of memory" in str(e):
            tqdm.write(f"CUDA out of memory error while processing {video_path}. Trying smaller batch size...")
            torch.cuda.empty_cache()  # Clear GPU memory to recover
        else:
            tqdm.write(f"Error processing video {video_path}: {e}")
        continue  # Skip this video and move to the next

    # Handle any other exceptions
    except Exception as e:
        tqdm.write(f"Error processing video {video_path}: {e}")
        torch.cuda.empty_cache()  # Clear GPU memory for recovery
        continue  # Skip this video and move to the next



--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\extract_segments.py
File type: .py
# extract segments
import torch
import cv2
def extract_segments_from_video(video_path, segment_size=16, target_shape=(240, 320), frame_skip=10):
    """
    Extracts video segments with frame skipping and efficient memory usage.

    Args:
        video_path (str): Path to the video file.
        segment_size (int): Number of frames per segment.
        target_shape (tuple): Target resolution (height, width) for frames.
        frame_skip (int): Number of frames to skip during extraction.

    Returns:
        list: List of segments, where each segment is a tensor of shape [C, T, H, W].
    """
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Unable to open video file: {video_path}")
        
        segments = []
        frames = []
        frame_count = 0

        while True:
            success, frame = cap.read()
            if not success:
                break

            # Skip frames based on frame_skip value
            if frame_count % frame_skip != 0:
                frame_count += 1
                continue

            # Resize frame and convert to tensor
            frame = cv2.resize(frame, target_shape)
            frame_tensor = torch.tensor(frame).permute(2, 0, 1).float() / 255.0  # Normalize to [0, 1]
            frames.append(frame_tensor)

            # Create a segment if enough frames are collected
            if len(frames) == segment_size:
                segments.append(torch.stack(frames, dim=1))  # Stack frames [C, T, H, W]
                frames = []

            frame_count += 1

        # Handle leftover frames (pad to segment size)
        if frames:
            while len(frames) < segment_size:
                frames.append(torch.zeros_like(frames[0]))  # Pad with black frames
            segments.append(torch.stack(frames, dim=1))

        cap.release()
        return segments

    except Exception as e:
        cap.release()
        print(f"Error processing video {video_path}: {e}")
        raise e

if __name__ == "__main__":
    file = r"E:\MIL\Dataset\Anomaly-Videos\Anomaly-Videos-Part-2\Explosion\Explosion001_x264.mp4"
    file2=r"E:\MIL\Dataset\Anomaly-Videos\Anomaly-Videos-Part-1\Abuse\Abuse001_x264.mp4"
    list_segment = extract_segments_from_video(file, segment_size=16, target_shape=(240, 320), frame_skip=2)
    list_segment_2 = extract_segments_from_video(file2, segment_size=16, target_shape=(240, 320), frame_skip=2)
    print(len(list_segment))
    for idx, segment in enumerate(list_segment_2):
        print(f"Segment {idx + 1} shape: {segment.shape}")
        break  # Only print the first segment for testing




--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\device check.py
File type: .py
import torch
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\unprocessed_video_data_extraction.py
File type: .py
import cv2
import torch
import os
import pandas as pd
from tqdm import tqdm
from torch.amp import autocast
from extract_features import extract_features

import cv2
import torch

def extract_segments_from_video(video_path, segment_size=16, target_shape=(128, 128), frame_skip=10):
    """
    Extracts video segments with frame skipping and efficient memory usage.

    Args:
        video_path (str): Path to the video file.
        segment_size (int): Number of frames per segment.
        target_shape (tuple): Target resolution (height, width) for frames.
        frame_skip (int): Number of frames to skip during extraction.

    Returns:
        list: List of segments, where each segment is a tensor of shape [C, T, H, W].
    """
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Unable to open video file: {video_path}")
        
        segments = []
        frames = []
        frame_count = 0

        while True:
            success, frame = cap.read()
            if not success:
                break

            # Skip frames based on frame_skip value
            if frame_count % frame_skip != 0:
                frame_count += 1
                continue

            # Resize frame and convert to tensor
            frame = cv2.resize(frame, target_shape)
            frame_tensor = torch.tensor(frame).permute(2, 0, 1).float() / 255.0  # Normalize to [0, 1]
            frames.append(frame_tensor)

            # Create a segment if enough frames are collected
            if len(frames) == segment_size:
                segments.append(torch.stack(frames, dim=1))  # Stack frames [C, T, H, W]
                frames = []

            frame_count += 1

        # Handle leftover frames (pad to segment size)
        if frames:
            while len(frames) < segment_size:
                frames.append(torch.zeros_like(frames[0]))  # Pad with black frames
            segments.append(torch.stack(frames, dim=1))

        cap.release()
        return segments

    except Exception as e:
        cap.release()
        print(f"Error processing video {video_path}: {e}")
        raise e

# File paths
file_path = r"E:\MIL\weakly_supervised_metadata.csv"  # Path to metadata
output_dir = 'processed_features'  # Directory for saved features
os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist

# Load metadata
if not os.path.exists(file_path):
    raise FileNotFoundError(f'Metadata file not found at {file_path}')
metadata = pd.read_csv(file_path)

# Get list of already processed files
processed_files = set(f.split('_features.pt')[0] for f in os.listdir(output_dir) if f.endswith('_features.pt'))

# Filter unprocessed videos
unprocessed_metadata = metadata[~metadata['file_path'].apply(lambda x: os.path.splitext(os.path.basename(x))[0]).isin(processed_files)]

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Batch size for processing
batch_size = 4

# Process unprocessed videos
for idx, row in tqdm(unprocessed_metadata.iterrows(), total=len(unprocessed_metadata), desc="Processing Unprocessed Videos"):
    video_path = row['file_path']
    label = row['label']

    try:
        if not os.path.exists(video_path):
            tqdm.write(f"Warning: Video file not found: {video_path}")
            continue

        tqdm.write(f"Starting video: {video_path}")

        # Extract segments
        segments = extract_segments_from_video(video_path)

        # Ensure all segments have consistent temporal size
        processed_segments = []
        for segment in segments:
            if segment.size(1) < 16:  # Pad if necessary
                pad_size = 16 - segment.size(1)
                padding = torch.zeros((segment.size(0), pad_size, segment.size(2), segment.size(3)))
                segment = torch.cat([segment, padding], dim=1)
            elif segment.size(1) > 16:  # Truncate if necessary
                segment = segment[:, :16, :, :]
            processed_segments.append(segment)
        segments = processed_segments

        # Divide into batches and extract features
        batches = [segments[i:i + batch_size] for i in range(0, len(segments), batch_size)]
        features = []
        for batch in batches:
            batch = torch.stack(batch).to(device)
            with autocast("cuda"):
                batch_features = extract_features(batch, device)
            if isinstance(batch_features, list):
                batch_features = torch.stack(batch_features)
            features.append(batch_features.cpu())

        features = torch.cat(features, dim=0)

        # Save features
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        torch.save({
            "features": features,
            "label": label
        }, os.path.join(output_dir, f"{video_name}_features.pt"))

        torch.cuda.empty_cache()
        tqdm.write(f"Processed video: {video_name}, Label: {label}")

    except RuntimeError as e:
        if "out of memory" in str(e):
            tqdm.write(f"CUDA out of memory error on {video_path}. Skipping...")
            torch.cuda.empty_cache()
        else:
            tqdm.write(f"Runtime error on {video_path}: {e}")
        continue

    except Exception as e:
        tqdm.write(f"Error processing video {video_path}: {e}")
        continue


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\Feature_Dataset_Class.py
File type: .py
# Code for Feature Dataset

from torch.utils.data import Dataset
import torch

class FeatureDataset(Dataset):
    def __init__(self, file_paths):
        self.file_paths = file_paths

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        data = torch.load(self.file_paths[idx],weights_only=False)
        # Ensure features are cast to float32
        features = data["features"].to(dtype=torch.float32)
        label = torch.tensor(data["label"]).float()
        return features, label

--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.gitignore
File type: 
/venv
/__pycache__
/device check.py
/unprocessed_video_data_extraction.py

--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\loss_function.py
File type: .py
#loss_function.py

import torch
import torch.nn.functional as F

def ranking_loss(scores, labels, batch_size, lamda_sparsity=8e-5, lamda_smooth=8e-5, margin=1):
    """
    Ranking loss for weakly-supervised MIL anomaly detection.

    Parameters:
    - scores (torch.Tensor): Predicted scores for all segments. Shape: [batch_size * num_segments].
    - labels (torch.Tensor): Binary labels for videos. Shape: [batch_size].
    - batch_size (int): Number of videos per batch.
    - lamda_sparsity (float): Weight for sparsity loss. Default: 1e-6.
    - lamda_smooth (float): Weight for smoothness loss. Default: 1e-6.
    - margin (float): Margin for ranking loss. Default: 1.0.

    Returns:
    - torch.Tensor: The combined ranking loss.
    """
    num_segments = scores.shape[0] // batch_size  # Segments per video
    total_loss = 0.0  # Initialize cumulative loss

    for i in range(batch_size):
        # Extract scores for the current video
        video_scores = scores[i * num_segments : (i + 1) * num_segments]
        video_label = labels[i]
        # print(f'video_label:{video_label}')

        # Compute max scores for anomaly and normal cases
        max_anomalous = torch.tensor(float("-inf"), device=scores.device)
        max_normal = torch.tensor(float("-inf"), device=scores.device)

        if video_label == 0:  # Anomalous video
            max_anomalous = torch.max(video_scores)
            #print(f'max_anomalous:{max_anomalous}')
        elif video_label == 1:  # Normal video
            max_normal = torch.max(video_scores)
            #print(f'max_normal:{max_normal}')

        # Compute ranking loss: Ensuring valid conditions
        if max_anomalous != float("-inf") and max_normal != float("-inf"):
            rank_loss = F.relu(margin - max_anomalous + max_normal)
            total_loss += rank_loss

        # Sparsity loss: Encourage sparsity in scores
        sparsity_loss = lamda_sparsity * torch.sum(torch.sigmoid(video_scores))

        # Smoothness loss: Penalize abrupt changes between adjacent segments
        smoothness_loss = lamda_smooth * torch.sum(
            (torch.sigmoid(video_scores[1:]) - torch.sigmoid(video_scores[:-1])) ** 2
        )

        total_loss += sparsity_loss + smoothness_loss

    # Normalize by batch size
    return total_loss / batch_size

--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\split_data_train_val_test.py
File type: .py
import os
import random
import shutil
import torch
from torch.utils.data import Dataset, DataLoader

# Define paths
normal_dir = "normal_features"
anomaly_dir = "anomaly_features"

# Create output directories
output_dirs = {
    "train": {"normal": "split/train/normal", "anomaly": "split/train/anomalous"},
    "val": {"normal": "split/val/normal", "anomaly": "split/val/anomalous"},
    "test": {"normal": "split/test/normal", "anomaly": "split/test/anomalous"},
}

# Create the directories
for split, dirs in output_dirs.items():
    for key, path in dirs.items():
        os.makedirs(path, exist_ok=True)

# Load all files
normal_files = [os.path.join(normal_dir, f) for f in os.listdir(normal_dir) if f.endswith(".pt")]
anomaly_files = [os.path.join(anomaly_dir, f) for f in os.listdir(anomaly_dir) if f.endswith(".pt")]

# Shuffle the files
random.seed(42)
random.shuffle(normal_files)
random.shuffle(anomaly_files)

# Compute splits
split_ratios = {"train": 0.7, "val": 0.15, "test": 0.15}

def split_files(files, ratios):
    n_train = int(len(files) * ratios["train"])
    n_val = int(len(files) * ratios["val"])
    train = files[:n_train]
    val = files[n_train:n_train + n_val]
    test = files[n_train + n_val:]
    return train, val, test

normal_train, normal_val, normal_test = split_files(normal_files, split_ratios)
anomaly_train, anomaly_val, anomaly_test = split_files(anomaly_files, split_ratios)

# Copy files to respective directories
for split, files in [("train", normal_train), ("val", normal_val), ("test", normal_test)]:
    for file in files:
        shutil.copy(file, output_dirs[split]["normal"])

for split, files in [("train", anomaly_train), ("val", anomaly_val), ("test", anomaly_test)]:
    for file in files:
        shutil.copy(file, output_dirs[split]["anomaly"])

print("Dataset split completed.")
print(f"Train: Normal = {len(normal_train)}, Anomaly = {len(anomaly_train)}")
print(f"Validation: Normal = {len(normal_val)}, Anomaly = {len(anomaly_val)}")
print(f"Test: Normal = {len(normal_test)}, Anomaly = {len(anomaly_test)}")

def pad_to_max_segments(feature_tensor, max_segments):
    """
    Pads the feature tensor to the max_segments along the batch/segment dimension.
    
    Args:
        feature_tensor (torch.Tensor): Input tensor of shape [B, ...].
        max_segments (int): Maximum segments to pad to.
        
    Returns:
        torch.Tensor: Padded tensor of shape [max_segments, ...].
    """
    current_segments = feature_tensor.size(0)
    if current_segments < max_segments:
        padding = torch.zeros((max_segments - current_segments, *feature_tensor.shape[1:]), device=feature_tensor.device)
        feature_tensor = torch.cat([feature_tensor, padding], dim=0)
    return feature_tensor

class FeatureDataset(Dataset):
    def __init__(self, normal_files, anomaly_files, max_segments):
        self.normal_files = normal_files
        self.anomaly_files = anomaly_files
        self.max_segments = max_segments  # Max segments for padding

    def __len__(self):
        return max(len(self.normal_files), len(self.anomaly_files))

    def __getitem__(self, idx):
        normal_idx = idx % len(self.normal_files)
        anomaly_idx = idx % len(self.anomaly_files)

        normal_feature = torch.load(self.normal_files[normal_idx])["features"]
        anomaly_feature = torch.load(self.anomaly_files[anomaly_idx])["features"]

        # Apply padding
        normal_feature = pad_to_max_segments(normal_feature, self.max_segments)
        anomaly_feature = pad_to_max_segments(anomaly_feature, self.max_segments)

        return normal_feature, anomaly_feature

# Create DataLoaders for training, validation, and testing
def create_dataloader(normal_dir, anomaly_dir, batch_size,max_segments):
    normal_files = [os.path.join(normal_dir, f) for f in os.listdir(normal_dir) if f.endswith(".pt")]
    anomaly_files = [os.path.join(anomaly_dir, f) for f in os.listdir(anomaly_dir) if f.endswith(".pt")]
    dataset = FeatureDataset(normal_files, anomaly_files,max_segments)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Directories for train, val, and test splits
train_normal_dir = "split/train/normal"
train_anomalous_dir = "split/train/anomalous"
val_normal_dir = "split/val/normal"
val_anomalous_dir = "split/val/anomalous"
test_normal_dir = "split/test/normal"
test_anomalous_dir = "split/test/anomalous"

# Max segments for each split
max_segments_train = 1018  # Max for training
max_segments_val = 253    # Max for validation
max_segments_test = 281   # Max for testing

# Batch size
batch_size = 4

# Create DataLoaders
train_loader = create_dataloader(train_normal_dir, train_anomalous_dir, batch_size, max_segments_train)
val_loader = create_dataloader(val_normal_dir, val_anomalous_dir, batch_size, max_segments_val)
test_loader = create_dataloader(test_normal_dir, test_anomalous_dir, batch_size, max_segments_test)

# Print DataLoader sizes
print(f"Train Loader: {len(train_loader)} batches")
print(f"Validation Loader: {len(val_loader)} batches")
print(f"Test Loader: {len(test_loader)} batches")






--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\epochs_training_testing.py
File type: .py
# epoch_training_test.py

import torch
from torch import nn, optim
from torch.amp import autocast, GradScaler
from split_data_train_val_test import create_dataloader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import numpy as np
import warnings
from sklearn.metrics import precision_recall_curve,f1_score
import matplotlib.pyplot as plt

# Assuming SequentialMILModel is already implemented
from Sequential_Model import SequentialMILModel
from loss_function import ranking_loss

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1. Initialize Model
model = SequentialMILModel(input_dim=2048).to(device)

# 2. Define Optimizer
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Scheduler: Reduces learning rate when validation loss plateaus
scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)

# 3. Define Loss Function
criterion = ranking_loss  # Replace with your ranking loss function

# 4. Gradient Scaler for Mixed Precision
scaler = GradScaler()

# Data Loaders
# Directories for train, val, and test splits
train_normal_dir = "split/train/normal"
train_anomalous_dir = "split/train/anomalous"
val_normal_dir = "split/val/normal"
val_anomalous_dir = "split/val/anomalous"
test_normal_dir = "split/test/normal"
test_anomalous_dir = "split/test/anomalous"

# Max segments for each split
max_segments_train = 1018  # Max for training
max_segments_val = 253   # Max for validation
max_segments_test = 281   # Max for testing

# Batch size
batch_size = 4

# Create DataLoaders
train_loader = create_dataloader(train_normal_dir, train_anomalous_dir, batch_size, max_segments_train)
val_loader = create_dataloader(val_normal_dir, val_anomalous_dir, batch_size, max_segments_val)
test_loader=create_dataloader(test_normal_dir,test_anomalous_dir,batch_size, max_segments_test)

def print_model_weights(model, epoch):
    """
    #Prints the weights of the model's layers.
    
    Args:
        model (torch.nn.Module): Trained model.
        epoch (int): Current epoch number.
    """
    print(f"\nWeights after Epoch {epoch + 1}:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Layer: {name}")
            print(f"Weight: {param.data}")
            print(f"Weight Shape: {param.data.shape}")
            print(f"Weight Mean: {param.data.mean():.6f}, Weight Std: {param.data.std():.6f}\n")

# Training Loop
def train_epoch(train_loader, model, optimizer, criterion, device, scaler, batch_size):
    model.train()  # Set model to training mode
    total_loss = 0.0
    all_labels=[]
    all_probs=[]

    for normal_features, anomalous_features in tqdm(train_loader, desc="Training"):
        # Combine features
        if normal_features.dim() > 2:            
            normal_features = normal_features.squeeze(-1).squeeze(-1).squeeze(-1)  # Remove [1, 1, 1] at the end
            normal_features = normal_features.view(-1, normal_features.size(-1))          # Reshape to [n * pad seg, 2048]
            #print(f"Normal features reshaped: {normal_features.shape}")
        if anomalous_features.dim() > 2:
            anomalous_features=anomalous_features.squeeze(-1).squeeze(-1).squeeze(-1)
            anomalous_features = anomalous_features.view(-1,anomalous_features.size(-1))
            #print(f"Anomalous features reshaped: {anomalous_features.shape}")
        
        normal_features = normal_features.to(device)
        anomalous_features = anomalous_features.to(device)
        inputs = torch.cat((normal_features, anomalous_features), dim=0)

        # Labels: 1 for normal, 0 for anomalous
        labels = torch.cat((
            torch.ones(len(normal_features), 1).to(device),  # Normal
            torch.zeros(len(anomalous_features), 1).to(device)  # Anomalous
        ), dim=0)

        optimizer.zero_grad()

        # Mixed precision forward pass
        with autocast(device_type="cuda" if torch.cuda.is_available() else "cpu",enabled=True):
            outputs = model(inputs)
            #print(f"Model outputs (first 10): {outputs[:10]}")
            loss = criterion(outputs, labels, batch_size)

        #print(f"Loss value: {loss.item()}")
        # Backward pass with scaled gradients
        scaler.scale(loss).backward()
        #print("Inspecting Gradients:")
        # for name, param in model.named_parameters():
        #     if param.requires_grad:
        #         if param.grad is not None:
        #             print(f"{name} - Grad Mean: {param.grad.mean()}, Grad Std: {param.grad.std()}")
        #         else:
        #             print(f"{name} - Grad is None")
        scaler.step(optimizer)
        scaler.update()

        

        total_loss += loss.item()
        all_probs.extend(outputs.squeeze().cpu().tolist())
        all_labels.extend(labels.squeeze().cpu().tolist())

    avg_loss = total_loss / len(val_loader)
    print(f"Training Loss: {avg_loss:.4f}")

    # Compute optimal threshold
    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_threshold_index = np.argmax(f1_scores)
    best_threshold = thresholds[best_threshold_index]
    print(f"Best Threshold: {best_threshold:.4f}, Best F1 Score: {f1_scores[best_threshold_index]:.4f}")

    plt.plot(recall, precision)
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.savefig(f"train_precision_recall_curve_epoch_{epoch}.png")  # Save to file
    plt.close()  

    return avg_loss, best_threshold
    '''
    avg_loss = total_loss / len(train_loader)
    print(f"Training Loss: {avg_loss:.4f}")
    return avg_loss
    '''

# Validation Loop
def validate_epoch(val_loader, model, criterion, device, batch_size):
    model.eval()  # Set model to evaluation mode
    total_loss = 0.0
    all_labels = []
    all_probs = []
    with torch.no_grad():  # No gradient calculation
        for normal_features, anomalous_features in tqdm(test_loader, desc="Validation"):
            # Combine features
            if normal_features.dim() > 2:            
                normal_features = normal_features.squeeze(-1).squeeze(-1).squeeze(-1)  # Remove [1, 1, 1] at the end
                normal_features = normal_features.view(-1, normal_features.size(-1))   # Reshape to [n * pad seg, 2048]
                #print(f"Normal features reshaped: {normal_features.shape}")
            if anomalous_features.dim() > 2:
                anomalous_features=anomalous_features.squeeze(-1).squeeze(-1).squeeze(-1)
                anomalous_features = anomalous_features.view(-1,anomalous_features.size(-1))
                #print(f"Anomalous features reshaped: {anomalous_features.shape}")
            
            normal_features = normal_features.to(device)
            anomalous_features = anomalous_features.to(device)
            inputs = torch.cat((normal_features, anomalous_features), dim=0)

            # Labels: 1 for normal, 0 for anomalous
            labels = torch.cat((
                torch.ones(len(normal_features), 1).to(device),  # Normal
                torch.zeros(len(anomalous_features), 1).to(device)  # Anomalous
            ), dim=0)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels,batch_size)
            total_loss += loss.item()
            # Collect probabilities and labels
            all_probs.extend(outputs.squeeze().cpu().tolist())
            all_labels.extend(labels.squeeze().cpu().tolist())

    avg_loss = total_loss / len(val_loader)
    print(f"Validation Loss: {avg_loss:.4f}")

    # Compute optimal threshold
    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_threshold_index = np.argmax(f1_scores)
    best_threshold = thresholds[best_threshold_index]
    print(f"Best Threshold: {best_threshold:.4f}, Best F1 Score: {f1_scores[best_threshold_index]:.4f}")
        # Plot Precision-Recall Curve
    plt.plot(recall, precision)
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curve")
    plt.savefig(f"precision_recall_curve_epoch_{epoch}.png")  # Save to file
    plt.close()  
    return avg_loss, best_threshold

def test_epoch(test_loader, model, criterion, device, batch_size):
    model.eval()  # Set model to evaluation mode
    total_loss = 0.0
    all_labels = []
    all_probs = []
    with torch.no_grad():  # No gradient calculation
        for normal_features, anomalous_features in tqdm(test_loader, desc="Test"):
            # Combine features
            if normal_features.dim() > 2:            
                normal_features = normal_features.squeeze(-1).squeeze(-1).squeeze(-1)  # Remove [1, 1, 1] at the end
                normal_features = normal_features.view(-1, normal_features.size(-1))   # Reshape to [n * pad seg, 2048]
                #print(f"Normal features reshaped: {normal_features.shape}")
            if anomalous_features.dim() > 2:
                anomalous_features=anomalous_features.squeeze(-1).squeeze(-1).squeeze(-1)
                anomalous_features = anomalous_features.view(-1,anomalous_features.size(-1))
                #print(f"Anomalous features reshaped: {anomalous_features.shape}")
            
            normal_features = normal_features.to(device)
            anomalous_features = anomalous_features.to(device)
            inputs = torch.cat((normal_features, anomalous_features), dim=0)

            # Labels: 1 for normal, 0 for anomalous
            labels = torch.cat((
                torch.ones(len(normal_features), 1).to(device),  # Normal
                torch.zeros(len(anomalous_features), 1).to(device)  # Anomalous
            ), dim=0)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels,batch_size)
            total_loss += loss.item()
                       # Collect probabilities and labels
            all_probs.extend(outputs.squeeze().cpu().tolist())
            all_labels.extend(labels.squeeze().cpu().tolist())

    avg_loss = total_loss / len(val_loader)
    print(f"test Loss: {avg_loss:.4f}")

    # Compute optimal threshold
    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_threshold_index = np.argmax(f1_scores)
    best_threshold = thresholds[best_threshold_index]
    print(f"Best Threshold: {best_threshold:.4f}, Best F1 Score: {f1_scores[best_threshold_index]:.4f}")

    return avg_loss, best_threshold

num_epochs = 10
batch_size = 4  # Keep consistent with DataLoader


for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    with warnings.catch_warnings(): 
        warnings.simplefilter("ignore", category=FutureWarning)
        train_loss,best_train_threshold = train_epoch(train_loader, model, optimizer, criterion, device, scaler, batch_size)
        val_loss,best_threshold = validate_epoch(val_loader, model, criterion, device, batch_size)
        test_loss=test_epoch(test_loader,model, criterion, device, batch_size)
        scheduler.step(val_loss)
    

    # Print model weights
    # print_model_weights(model, epoch)
print(f'best_train_threshold:{best_train_threshold}')
print(f'best_threshold:{best_threshold}')

torch.save(model.state_dict(), "trained_model.pth")

if __name__ == "__main__":
    import torch
    import cv2
    from extract_segments import extract_segments_from_video  # Function to divide videos into segments
    from extract_features import extract_features  # Function to extract features from segments
    from Sequential_Model import SequentialMILModel  # Your MIL model
    device=torch.device('cuda')

    # best_train_threshold=9.5367431640625e-07
    # best_threshold= 0.00028432568069547415
    def infer_video(video_path, model, device):
        """
        Perform inference on a single video to predict its label (0 or 1).

        Args:
            video_path (str): Path to the video file.
            model (torch.nn.Module): Trained MIL model.
            device (torch.device): Device for computation (CPU or GPU).

        Returns:
            float: Predicted probability (0.0 to 1.0) of being anomalous.
        """
        #try:
            # Step 1: Extract video segments
        segments = extract_segments_from_video(video_path, segment_size=16, target_shape=(240, 320), frame_skip=5)
        if not segments:
            raise ValueError("No segments were extracted from the video.")

        # Step 2: Extract features for each segment
        model.eval()
        with torch.no_grad():
            for segment in segments:
                segment = segment.unsqueeze(0).to(device)  # Add batch dimension
                segment_features = extract_features(segment, device)
            print(f'segment features: {segment_features[0].shape}')
        
        # Combine features from all segments
        segment_features = torch.cat(segment_features, dim=0).to(device)  # Shape: [num_segments, feature_dim]
        if segment_features.dim() > 2:
            segment_features=segment_features.squeeze(-1).squeeze(-1).squeeze(-1)
            segment_features = segment_features.view(-1,segment_features.size(-1))

        # Step 3: Predict using the trained MIL model
        with torch.no_grad():
            scores = model(segment_features)
            print(f'torch.mean(scores):{torch.mean(scores)}')
            print(f'torch.max(scores):{torch.max(scores)}')
              # Aggregate scores across segments
            predicted_probability = torch.mean(scores).item()
        
        return predicted_probability 

        #except Exception as e:
            #print(f"Error during inference: {e}")
            #return None

    # File path for the video
    video_path = r"E:\MIL\Dataset\Normal-Videos\Normal_Videos038_x264.mp4" # Replace with the actual path to the new video

    # Device setup
    device = torch.device('cuda')

    # Load the trained MIL model
    model = SequentialMILModel(input_dim=2048).to(device)
    model.load_state_dict(torch.load("trained_model.pth",weights_only=True))  # Replace with the actual path to your model weights

    # Perform inference
    predicted_probability = infer_video(video_path, model, device)
    if predicted_probability is not None:
        predicted_label = 0 if predicted_probability > best_threshold else 1
        print(f"Predicted Probability of being Anomalous: {predicted_probability}")
        print(f"best_threshold:{best_threshold}")
        print(f"Predicted Label: {predicted_label}")
    
    



'''
from Sequential_Model import SequentialMILModel
import torch
from torch import nn
from split_data_train_val_test import DataPreparation
from loss_function import combined_loss
from torch.amp import autocast 
# Automatically switches between float16 and float32 precision during 
# training for faster computation.
from torch.amp import GradScaler
# Scales gradients to prevent underflow when using float16. not go to 0
# Initialize Model, Optimizer, and Scaler
from tqdm import tqdm
from torch.optim.lr_scheduler import ReduceLROnPlateau


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SequentialMILModel(input_dim=2048).to(device)
# Initialize optimizer and scheduler
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)
scheduler = ReduceLROnPlateau(
    optimizer,
    mode="min",         # Minimize the monitored metric (e.g., loss)
    factor=0.1,         # Reduce LR by a factor of 0.1
    patience=5,         # Wait 5 epochs for improvement before reducing LR
    verbose=True        # Print updates when LR changes
)
scaler = GradScaler()  # Gradient scaler for mixed precision

# Path to the directory containing processed feature files
processed_features_dir = "processed_features"

# Initialize the DataPreparation class
data_prep = DataPreparation(processed_features_dir)
# Split the dataset
data_prep.split_dataset(test_size=0.4, val_size=0.5)
# Create DataLoaders
train_loader, val_loader,test_loader = data_prep.get_data_loaders(batch_size=1)


# Training Loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    train_loss = 0.0

    # Training Progress Bar
    train_progress = tqdm(train_loader, desc=f"Epoch {epoch + 1} - Training", leave=False)
    ce_loss=0
    sparsity_loss=0
    smoothness_loss=0
    for features, label in train_progress:
        features = features.squeeze(0).to(device)  # [num_segments, feature_dim] [Segments, 2048]
        label = label.to(device)  # [1]

        optimizer.zero_grad()  # Reset gradients

        # Forward pass with autocast for mixed precision
        with autocast(device_type='cuda', dtype=torch.float32):
            scores = model(features)  # Get raw logits from the model
            scores=torch.mean(scores).unsqueeze(0)
            loss = combined_loss(scores, label) 


        # Backward pass with gradient scaling
        scaler.scale(loss).backward()  # Scale gradients for mixed precision
        scaler.step(optimizer)  # Update model parameters
        scaler.update()  # Update gradient scaler for next step

        train_loss += loss.item()  # Accumulate training loss


        # Update training progress bar with current loss
        train_progress.set_postfix(loss=loss.item())
        ce_loss+= nn.BCEWithLogitsLoss()(scores, label)
        sparsity_loss+= 1e-6 * torch.sum(torch.sigmoid(scores))
        smoothness_loss+= 1e-6 * torch.sum((torch.sigmoid(scores[1:]) - torch.sigmoid(scores[:-1])) ** 2)
    print(f"BCE={ce_loss.item()}, Sparsity={sparsity_loss.item()}, Smoothness={smoothness_loss.item()}")
    print(f'train loss - epoch:{train_loss} - {epoch}')
    
print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}")'''


'''
    # Validation Loop
    model.eval()  # Set model to evaluation mode
    val_loss = 0.0
    val_progress = tqdm(val_loader, desc=f"Epoch {epoch + 1} - Validation", leave=False)
    with torch.no_grad():  # Disable gradient calculations during validation
        i=0
        for features, label in val_progress:
            
            features = features.squeeze(0).to(device).to(dtype=torch.float32)  # Convert to float32
            label = label.to(device)

            # Forward pass
            scores = model(features)
            scores=torch.mean(scores).unsqueeze(0)
            loss = combined_loss(scores, label)  # Calculate validation loss
            print(f'validation_loss at iteration {i} is : {loss.item()}')
            val_loss += loss.item()
            i+=1
            print(i)

            # Update validation progress bar with current loss
            val_progress.set_postfix(loss=loss.item())
'''
    # Print epoch summary
    # print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}")
    # print(f"Val Loss: {val_loss:.4f}")

'''
# Test Loop
model.eval()
test_scores = []
test_labels = []
test_progress = tqdm(test_loader, desc="Testing")

with torch.no_grad():
    for features, label in test_progress:
        features = features.squeeze(0).to(device).to(dtype=torch.float32)
        label = label.to(device)
        scores = model(features)
        scores=torch.mean(scores).unsqueeze(0)
        test_scores.append(scores.item())
        test_labels.append(label.item())

# Calculate Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score

# print(test_scores)

predictions = [1 if score > 0.5 else 0 for score in test_scores]
accuracy = accuracy_score(test_labels, predictions)
precision = precision_score(test_labels, predictions)
recall = recall_score(test_labels, predictions)

print(f"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")
'''

--------------------------------------------------
File End
--------------------------------------------------


Cosine Similarity: 0.6851, Euclidean Distance: 6.6719
Cosine Similarity: 0.7378, Euclidean Distance: 6.0117

Mean Cosine Similarity: 0.7461
Mean Euclidean Distance: 5.7396


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\unused_code.py
File type: .py
# epoch_training_test.py

import torch
from torch import nn, optim
from torch.amp import autocast, GradScaler
from split_data_train_val_test import create_dataloader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import numpy as np
from sklearn.metrics import precision_recall_curve,f1_score

# Assuming SequentialMILModel is already implemented
from Sequential_Model import SequentialMILModel
from loss_function import ranking_loss

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1. Initialize Model
model = SequentialMILModel(input_dim=2048).to(device)

# 2. Define Optimizer
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Scheduler: Reduces learning rate when validation loss plateaus
scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)

# 3. Define Loss Function
criterion = ranking_loss  # Replace with your ranking loss function

# 4. Gradient Scaler for Mixed Precision
scaler = GradScaler()

# Data Loaders
# Directories for train, val, and test splits
train_normal_dir = "split/train/normal"
train_anomalous_dir = "split/train/anomalous"
val_normal_dir = "split/val/normal"
val_anomalous_dir = "split/val/anomalous"
test_normal_dir = "split/test/normal"
test_anomalous_dir = "split/test/anomalous"

# Max segments for each split
max_segments_train = 887  # Max for training
max_segments_val = 675    # Max for validation
max_segments_test = 271   # Max for testing

# Batch size
batch_size = 4

# Create DataLoaders
train_loader = create_dataloader(train_normal_dir, train_anomalous_dir, batch_size, max_segments_train)
val_loader = create_dataloader(val_normal_dir, val_anomalous_dir, batch_size, max_segments_val)
test_loader=create_dataloader(test_normal_dir,test_anomalous_dir,batch_size, max_segments_test)

'''def print_model_weights(model, epoch):
    """
    Prints the weights of the model's layers.
    
    Args:
        model (torch.nn.Module): Trained model.
        epoch (int): Current epoch number.
    """
    print(f"\nWeights after Epoch {epoch + 1}:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Layer: {name}")
            print(f"Weight: {param.data}")
            print(f"Weight Shape: {param.data.shape}")
            print(f"Weight Mean: {param.data.mean():.6f}, Weight Std: {param.data.std():.6f}\n")

# Training Loop
def train_epoch(train_loader, model, optimizer, criterion, device, scaler, batch_size):
    model.train()  # Set model to training mode
    total_loss = 0.0

    for normal_features, anomalous_features in tqdm(train_loader, desc="Training"):
        # Combine features
        if normal_features.dim() > 2:            
            normal_features = normal_features.squeeze(-1).squeeze(-1).squeeze(-1)  # Remove [1, 1, 1] at the end
            normal_features = normal_features.view(-1, normal_features.size(-1))          # Reshape to [n * pad seg, 2048]
            #print(f"Normal features reshaped: {normal_features.shape}")
        if anomalous_features.dim() > 2:
            anomalous_features=anomalous_features.squeeze(-1).squeeze(-1).squeeze(-1)
            anomalous_features = anomalous_features.view(-1,anomalous_features.size(-1))
            #print(f"Anomalous features reshaped: {anomalous_features.shape}")
        
        normal_features = normal_features.to(device)
        anomalous_features = anomalous_features.to(device)
        inputs = torch.cat((normal_features, anomalous_features), dim=0)

        # Labels: 1 for normal, 0 for anomalous
        labels = torch.cat((
            torch.ones(len(normal_features), 1).to(device),  # Normal
            torch.zeros(len(anomalous_features), 1).to(device)  # Anomalous
        ), dim=0)

        optimizer.zero_grad()

        # Mixed precision forward pass
        with autocast(device_type="cuda" if torch.cuda.is_available() else "cpu",enabled=True):
            outputs = model(inputs)
            loss = criterion(outputs, labels, batch_size)

        # Backward pass with scaled gradients
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Training Loss: {avg_loss:.4f}")
    return avg_loss

# Validation Loop
def validate_epoch(val_loader, model, criterion, device, batch_size):
    model.eval()  # Set model to evaluation mode
    total_loss = 0.0

    with torch.no_grad():  # No gradient calculation
        for normal_features, anomalous_features in tqdm(val_loader, desc="Validation"):
            # Combine features
            if normal_features.dim() > 2:            
                normal_features = normal_features.squeeze(-1).squeeze(-1).squeeze(-1)  # Remove [1, 1, 1] at the end
                normal_features = normal_features.view(-1, normal_features.size(-1))   # Reshape to [n * pad seg, 2048]
                #print(f"Normal features reshaped: {normal_features.shape}")
            if anomalous_features.dim() > 2:
                anomalous_features=anomalous_features.squeeze(-1).squeeze(-1).squeeze(-1)
                anomalous_features = anomalous_features.view(-1,anomalous_features.size(-1))
                #print(f"Anomalous features reshaped: {anomalous_features.shape}")
            
            normal_features = normal_features.to(device)
            anomalous_features = anomalous_features.to(device)
            inputs = torch.cat((normal_features, anomalous_features), dim=0)

            # Labels: 1 for normal, 0 for anomalous
            labels = torch.cat((
                torch.ones(len(normal_features), 1).to(device),  # Normal
                torch.zeros(len(anomalous_features), 1).to(device)  # Anomalous
            ), dim=0)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels,batch_size)
            total_loss += loss.item()

    avg_loss = total_loss / len(val_loader)
    print(f"Validation Loss: {avg_loss:.4f}")
    return avg_loss

def test_epoch(test_loader, model, criterion, device, batch_size):
    model.eval()  # Set model to evaluation mode
    total_loss = 0.0
    all_labels = []
    all_probs = []
    with torch.no_grad():  # No gradient calculation
        for normal_features, anomalous_features in tqdm(test_loader, desc="Validation"):
            # Combine features
            if normal_features.dim() > 2:            
                normal_features = normal_features.squeeze(-1).squeeze(-1).squeeze(-1)  # Remove [1, 1, 1] at the end
                normal_features = normal_features.view(-1, normal_features.size(-1))   # Reshape to [n * pad seg, 2048]
                #print(f"Normal features reshaped: {normal_features.shape}")
            if anomalous_features.dim() > 2:
                anomalous_features=anomalous_features.squeeze(-1).squeeze(-1).squeeze(-1)
                anomalous_features = anomalous_features.view(-1,anomalous_features.size(-1))
                #print(f"Anomalous features reshaped: {anomalous_features.shape}")
            
            normal_features = normal_features.to(device)
            anomalous_features = anomalous_features.to(device)
            inputs = torch.cat((normal_features, anomalous_features), dim=0)

            # Labels: 1 for normal, 0 for anomalous
            labels = torch.cat((
                torch.ones(len(normal_features), 1).to(device),  # Normal
                torch.zeros(len(anomalous_features), 1).to(device)  # Anomalous
            ), dim=0)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels,batch_size)
            total_loss += loss.item()
                       # Collect probabilities and labels
            all_probs.extend(outputs.squeeze().cpu().tolist())
            all_labels.extend(labels.squeeze().cpu().tolist())

    avg_loss = total_loss / len(val_loader)
    print(f"Validation Loss: {avg_loss:.4f}")

    # Compute optimal threshold
    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_threshold_index = np.argmax(f1_scores)
    best_threshold = thresholds[best_threshold_index]
    print(f"Best Threshold: {best_threshold:.4f}, Best F1 Score: {f1_scores[best_threshold_index]:.4f}")

    return avg_loss, best_threshold

num_epochs = 10
batch_size = 4  # Keep consistent with DataLoader



for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    train_loss = train_epoch(train_loader, model, optimizer, criterion, device, scaler, batch_size)
    val_loss,best_threshold = validate_epoch(val_loader, model, criterion, device, batch_size)
    test_loss=test_epoch(test_loader,model, criterion, device, batch_size)
    scheduler.step(val_loss)
    
    # Print model weights
    # print_model_weights(model, epoch)

torch.save(model.state_dict(), "trained_model.pth")
'''
if __name__ == "__main__":
    import torch
    import cv2
    from extract_segments import extract_segments_from_video  # Function to divide videos into segments
    from extract_features import extract_features  # Function to extract features from segments
    from Sequential_Model import SequentialMILModel  # Your MIL model
    device=torch.device('cuda')
    best_train_threshold=9.5367431640625e-07
    best_threshold= 0.00028432568069547415

    def infer_video(video_path, model, device):
        """
        Perform inference on a single video to predict its label (0 or 1).

        Args:
            video_path (str): Path to the video file.
            model (torch.nn.Module): Trained MIL model.
            device (torch.device): Device for computation (CPU or GPU).

        Returns:
            float: Predicted probability (0.0 to 1.0) of being anomalous.
        """
        try:
            # Step 1: Extract video segments
            segments = extract_segments_from_video(video_path, segment_size=16, target_shape=(240, 320), frame_skip=5)
            if not segments:
                raise ValueError("No segments were extracted from the video.")

            # Step 2: Extract features for each segment
            model.eval()
            features = []
            with torch.no_grad():
                for segment in segments:
                    segment = segment.unsqueeze(0).to(device)  # Add batch dimension
                    segment_features = extract_features(segment, device)
            
            # Combine features from all segments
            segment_features = torch.cat(segment_features, dim=0).to(device)  # Shape: [num_segments, feature_dim]
            if segment_features.dim() > 2:
                segment_features=segment_features.squeeze(-1).squeeze(-1).squeeze(-1)
                segment_features = segment_features.view(-1,segment_features.size(-1))

            # Step 3: Predict using the trained MIL model
            with torch.no_grad():
                scores = model(segment_features)
                predicted_probability = torch.mean(scores).item()  # Aggregate scores across segments
            
            return predicted_probability

        except Exception as e:
            print(f"Error during inference: {e}")
            return None

    # File path for the video
    video_path = r'E:\MIL\Code\videos\walking.mp4'  # Replace with the actual path to the new video

    # Device setup
    device = torch.device('cuda')

    # Load the trained MIL model
    model = SequentialMILModel(input_dim=2048).to(device)
    model.load_state_dict(torch.load("trained_model.pth"))  # Replace with the actual path to your model weights

    # Perform inference
    predicted_probability = infer_video(video_path, model, device)
    if predicted_probability is not None:
        predicted_label = 0 if predicted_probability > best_threshold else 1
        print(f"Predicted Probability of being Anomalous: {predicted_probability:.4f}")
        print(f"Predicted Label: {predicted_label}")


'''--------
from Sequential_Model import SequentialMILModel
import torch
from torch import nn
from split_data_train_val_test import DataPreparation
from loss_function import combined_loss
from torch.amp import autocast 
# Automatically switches between float16 and float32 precision during 
# training for faster computation.
from torch.amp import GradScaler
# Scales gradients to prevent underflow when using float16. not go to 0
# Initialize Model, Optimizer, and Scaler
from tqdm import tqdm
from torch.optim.lr_scheduler import ReduceLROnPlateau


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SequentialMILModel(input_dim=2048).to(device)
# Initialize optimizer and scheduler
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)
scheduler = ReduceLROnPlateau(
    optimizer,
    mode="min",         # Minimize the monitored metric (e.g., loss)
    factor=0.1,         # Reduce LR by a factor of 0.1
    patience=5,         # Wait 5 epochs for improvement before reducing LR
    verbose=True        # Print updates when LR changes
)
scaler = GradScaler()  # Gradient scaler for mixed precision

# Path to the directory containing processed feature files
processed_features_dir = "processed_features"

# Initialize the DataPreparation class
data_prep = DataPreparation(processed_features_dir)
# Split the dataset
data_prep.split_dataset(test_size=0.4, val_size=0.5)
# Create DataLoaders
train_loader, val_loader,test_loader = data_prep.get_data_loaders(batch_size=1)


# Training Loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    train_loss = 0.0

    # Training Progress Bar
    train_progress = tqdm(train_loader, desc=f"Epoch {epoch + 1} - Training", leave=False)
    ce_loss=0
    sparsity_loss=0
    smoothness_loss=0
    for features, label in train_progress:
        features = features.squeeze(0).to(device)  # [num_segments, feature_dim] [Segments, 2048]
        label = label.to(device)  # [1]

        optimizer.zero_grad()  # Reset gradients

        # Forward pass with autocast for mixed precision
        with autocast(device_type='cuda', dtype=torch.float32):
            scores = model(features)  # Get raw logits from the model
            scores=torch.mean(scores).unsqueeze(0)
            loss = combined_loss(scores, label) 


        # Backward pass with gradient scaling
        scaler.scale(loss).backward()  # Scale gradients for mixed precision
        scaler.step(optimizer)  # Update model parameters
        scaler.update()  # Update gradient scaler for next step

        train_loss += loss.item()  # Accumulate training loss


        # Update training progress bar with current loss
        train_progress.set_postfix(loss=loss.item())
        ce_loss+= nn.BCEWithLogitsLoss()(scores, label)
        sparsity_loss+= 1e-6 * torch.sum(torch.sigmoid(scores))
        smoothness_loss+= 1e-6 * torch.sum((torch.sigmoid(scores[1:]) - torch.sigmoid(scores[:-1])) ** 2)
    print(f"BCE={ce_loss.item()}, Sparsity={sparsity_loss.item()}, Smoothness={smoothness_loss.item()}")
    print(f'train loss - epoch:{train_loss} - {epoch}')
    
print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}")'''


'''
    # Validation Loop
    model.eval()  # Set model to evaluation mode
    val_loss = 0.0
    val_progress = tqdm(val_loader, desc=f"Epoch {epoch + 1} - Validation", leave=False)
    with torch.no_grad():  # Disable gradient calculations during validation
        i=0
        for features, label in val_progress:
            
            features = features.squeeze(0).to(device).to(dtype=torch.float32)  # Convert to float32
            label = label.to(device)

            # Forward pass
            scores = model(features)
            scores=torch.mean(scores).unsqueeze(0)
            loss = combined_loss(scores, label)  # Calculate validation loss
            print(f'validation_loss at iteration {i} is : {loss.item()}')
            val_loss += loss.item()
            i+=1
            print(i)

            # Update validation progress bar with current loss
            val_progress.set_postfix(loss=loss.item())
'''
    # Print epoch summary
    # print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}")
    # print(f"Val Loss: {val_loss:.4f}")

'''
# Test Loop
model.eval()
test_scores = []
test_labels = []
test_progress = tqdm(test_loader, desc="Testing")

with torch.no_grad():
    for features, label in test_progress:
        features = features.squeeze(0).to(device).to(dtype=torch.float32)
        label = label.to(device)
        scores = model(features)
        scores=torch.mean(scores).unsqueeze(0)
        test_scores.append(scores.item())
        test_labels.append(label.item())

# Calculate Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score

# print(test_scores)

predictions = [1 if score > 0.5 else 0 for score in test_scores]
accuracy = accuracy_score(test_labels, predictions)
precision = precision_score(test_labels, predictions)
recall = recall_score(test_labels, predictions)

print(f"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")
'''

#loss_function.py

'''import torch
import torch.nn.functional as F

def ranking_loss(scores, labels, batch_size, lamda_sparsity=1e-6, lamda_smooth=1e-6, margin=0.5):
    """
    Ranking loss for weakly-supervised MIL anomaly detection.

    Parameters:
    - scores (torch.Tensor): Predicted scores for all segments. Shape: [batch_size * num_segments].
    - labels (torch.Tensor): Binary labels for videos. Shape: [batch_size].
    - batch_size (int): Number of videos per batch.
    - lamda_sparsity (float): Weight for sparsity loss. Default: 1e-6.
    - lamda_smooth (float): Weight for smoothness loss. Default: 1e-6.
    - margin (float): Margin for ranking loss. Default: 1.0.

    Returns:
    - torch.Tensor: The combined ranking loss.
    """
    num_segments = scores.shape[0] // batch_size  # Segments per video
    total_loss = 0.0  # Initialize cumulative loss

    for i in range(batch_size):
        # Extract scores for the current video
        video_scores = scores[i * num_segments : (i + 1) * num_segments]
        video_label = labels[i]
        # print(f'video_label:{video_label}')

        # Compute max scores for anomaly and normal cases
        max_anomalous = torch.tensor(float("-inf"), device=scores.device)
        max_normal = torch.tensor(float("-inf"), device=scores.device)

        if video_label == 0:  # Anomalous video
            max_anomalous = torch.max(video_scores)
            #print(f'max_anomalous:{max_anomalous}')
        elif video_label == 1:  # Normal video
            max_normal = torch.max(video_scores)
            #print(f'max_normal:{max_normal}')

        # Compute ranking loss: Ensuring valid conditions
        if max_anomalous != float("-inf") and max_normal != float("-inf"):
            rank_loss = F.relu(margin - max_anomalous + max_normal)
            total_loss += rank_loss

        # Sparsity loss: Encourage sparsity in scores
        sparsity_loss = lamda_sparsity * torch.sum(torch.sigmoid(video_scores))

        # Smoothness loss: Penalize abrupt changes between adjacent segments
        smoothness_loss = lamda_smooth * torch.sum(
            (torch.sigmoid(video_scores[1:]) - torch.sigmoid(video_scores[:-1])) ** 2
        )

        total_loss += sparsity_loss + smoothness_loss

    # Normalize by batch size
    return total_loss / batch_size'''


'''import torch
import torch.nn.functional as F

def ranking_loss(scores, labels, batch_size, lamda_sparsity=1e-6, lamda_smooth=1e-6, margin=1.0):
    """
    Ranking loss for weakly-supervised MIL anomaly detection.

    Parameters:
    - scores (torch.Tensor): Predicted scores for all segments. Shape: [batch_size * num_segments].
    - labels (torch.Tensor): Binary labels for videos. Shape: [batch_size].
    - batch_size (int): Number of videos per batch.
    - lamda_sparsity (float): Weight for sparsity loss. Default: 1e-6.
    - lamda_smooth (float): Weight for smoothness loss. Default: 1e-6.
    - margin (float): Margin for ranking loss. Default: 1.0.

    Returns:
    - torch.Tensor: The combined ranking loss.
    """
    num_segments = scores.shape[0] // batch_size  # Assume all videos have the same number of segments
    loss = torch.tensor(0.0, device=scores.device, requires_grad=True)  # Initialize loss

    for i in range(batch_size):
        # Extract scores for the current video
        video_scores = scores[i * num_segments : (i + 1) * num_segments]
        video_label = labels[i]

        # Compute the max score for the video
        max_score = torch.max(video_scores)

        if video_label == 0:  # Anomalous video
            max_anomalous = max_score
        elif video_label == 1:  # Normal video
            max_normal = max_score

        # Ranking loss: max_anomalous > max_normal + margin
        ranking_loss = F.relu(margin - max_anomalous + max_normal)
        loss += ranking_loss

        # Add sparsity and smoothness losses
        sparsity_loss = lamda_sparsity * torch.sum(torch.sigmoid(video_scores))
        smoothness_loss = lamda_smooth * torch.sum((torch.sigmoid(video_scores[1:]) - torch.sigmoid(video_scores[:-1])) ** 2)

        loss += sparsity_loss + smoothness_loss

    # Normalize by batch size
    return loss / batch_size
'''

'''import torch
from torch import nn
def combined_loss(scores, labels, lamda_sparsity=1e-6, lamda_smooth=1e-6):
    """
    Calculates the combined loss function for weakly-supervised MIL anomaly detection.

    The combined loss consists of:
    1. Binary Cross-Entropy Loss with Logits:
       - This is the classification loss that measures how well the model predicts the 
         anomaly scores for each segment. It internally applies the sigmoid function 
         to convert logits into probabilities before calculating the loss.
    
    2. Sparsity Loss:
       - This encourages sparsity in anomaly predictions by penalizing the sum of 
         predicted probabilities across all segments. It helps the model focus on 
         a small number of segments likely to contain anomalies.

    3. Smoothness Loss:
       - This penalizes abrupt changes in predictions across adjacent segments 
         to ensure temporal smoothness in the predicted anomaly scores.

    Parameters:
    - scores (torch.Tensor): Predicted logits from the model. Shape: [num_segments].
    - labels (torch.Tensor): Ground truth labels. Shape: [num_segments].
    - lamda_sparsity (float): Weight for sparsity loss. Default: 8e-5.
    - lamda_smooth (float): Weight for smoothness loss. Default: 8e-5.

    Returns:
    - torch.Tensor: The combined loss value.
    """
    # Binary Cross-Entropy Loss with Logits (handles sigmoid internally)
    #ce_loss = nn.BCEWithLogitsLoss()(scores, labels)

    # Sparsity Loss: Penalizes the sum of predicted probabilities (encourages sparse anomaly detection)
    # sparsity_loss = lamda_sparsity * torch.sum(torch.sigmoid(scores))

    # Smoothness Loss: Penalizes abrupt changes in adjacent segment predictions
    # smoothness_loss = lamda_smooth * torch.sum((torch.sigmoid(scores[1:]) - torch.sigmoid(scores[:-1])) ** 2)

    # Combined loss
    device=torch.device('cuda' if torch.cuda.is_available() else "cpu")
    pos_weight = torch.tensor([1.5]).to(device)
    bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    ce_loss = bce_loss(scores, labels)
    sparsity_loss = lamda_sparsity * torch.sum(torch.sigmoid(scores))
    smoothness_loss = lamda_smooth * torch.sum((torch.sigmoid(scores[1:]) - torch.sigmoid(scores[:-1])) ** 2)

    

    return ce_loss + sparsity_loss + smoothness_loss'''

--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\find_max_segments.py
File type: .py
import torch
import os

def find_max_segments(data_dir):
    """
    Finds the maximum number of segments in the feature tensors for the given directory.

    Args:
        data_dir (str): Path to the directory containing feature files.

    Returns:
        int: Maximum number of segments.
        str: File name of the tensor with the max segments.
    """
    max_segments = 0
    max_file = None

    feature_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(".pt")]

    for file in feature_files:
        features = torch.load(file)["features"]  # Load features
        num_segments = features.shape[0]  # Number of segments is the first dimension (B)
        if num_segments > max_segments:
            max_segments = num_segments
            max_file = file

    return max_segments, max_file

# Directories for splits
splits = {
    "train": {"normal": r"E:\MIL\Code\split\train\normal", "anomaly":r"E:\MIL\Code\split\train\anomalous"},
    "val": {"normal": r"E:\MIL\Code\split\val\normal", "anomaly": r"E:\MIL\Code\split\val\anomalous"},
    "test": {"normal": r"E:\MIL\Code\split\test\normal", "anomaly": r"E:\MIL\Code\split\test\anomalous"}
}

# Find maximum segments for each split
for split_name, dirs in splits.items():
    for label, dir_path in dirs.items():
        max_segments, max_file = find_max_segments(dir_path)
        print(f"Max segments in {split_name} {label}: {max_segments} (File: {max_file})")

--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\Sequential_Model.py
File type: .py

import torch.nn as nn

class SequentialMILModel(nn.Module):
    def __init__(self, input_dim=2048, dropout_rate=0.3):
        super(SequentialMILModel, self).__init__()
        print("Updated SequentialMILModel Initialized")

        # Layer configuration: 2048 -> 1024 -> 512 -> 256 -> 128 -> 64 -> 32 -> 1
        self.fc1 = nn.Linear(input_dim, 1024)  # 2048 -> 1024
        self.bn1 = nn.BatchNorm1d(1024) if 1024 > 1 else None

        self.fc2 = nn.Linear(1024, 512)         # 1024 -> 512
        self.bn2 = nn.BatchNorm1d(512)

        self.fc3 = nn.Linear(512, 256)          # 512 -> 256
        self.bn3 = nn.BatchNorm1d(256)

        self.fc4 = nn.Linear(256, 128)          # 256 -> 128
        self.bn4 = nn.BatchNorm1d(128)

        self.fc5 = nn.Linear(128, 64)           # 128 -> 64
        self.bn5 = nn.BatchNorm1d(64)

        self.fc6 = nn.Linear(64, 32)            # 64 -> 32
        self.bn6 = nn.BatchNorm1d(32)

        self.fc7 = nn.Linear(32, 1)             # 32 -> 1

        self.dropout = nn.Dropout(dropout_rate)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)
        self.sigmoid = nn.Sigmoid()
        self.swish = nn.SiLU()
        self.relu = nn.ReLU()

        # Initialize weights
        self._initialize_weights()

    def forward(self, x):
        # Handle multi-dimensional inputs
        if x.dim() > 2:
            x = x.squeeze(-1).squeeze(-1).squeeze(-1)
            x = x.view(-1, x.size(-1))

        # First fully connected layer
        x = self.fc1(x)
        x = self.relu(x)

        # Second fully connected layer
        x = self.fc2(x)
        x = self.relu(x)

        # Third fully connected layer
        x = self.fc3(x)
        x = self.relu(x)

        # Fourth fully connected layer
        x = self.fc4(x)
        x = self.relu(x)

        # Fifth fully connected layer
        x = self.fc5(x)
        x = self.relu(x)

        # Sixth fully connected layer
        x = self.fc6(x)
        x = self.relu(x)

        # Final layer
        x = self.fc7(x)
        x = self.sigmoid(x)  # Output in range [0, 1]

        return x.squeeze(-1)

    # def _initialize_weights(self):
    #     for layer in self.modules():
    #         if isinstance(layer, nn.Linear):
    #             nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
    #             nn.init.constant_(layer.bias, 0)
    #         elif isinstance(layer, nn.BatchNorm1d):
    #             nn.init.constant_(layer.weight, 1)
    #             nn.init.constant_(layer.bias, 0)




# class SequentialMILModel(nn.Module):
#     def __init__(self, input_dim=2048, hidden_dim=512):
#         super(SequentialMILModel, self).__init__()
#         print("Complex SequentialMILModel Initialized")

#         # Layer configuration: 2048 -> 512 -> 32 -> 1
        
#         self.fc1 = nn.Linear(input_dim, hidden_dim)  # 2048 -> 512
#         self.bn1 = nn.BatchNorm1d(hidden_dim)
        
#         self.fc2 = nn.Linear(hidden_dim, 32)         # 512 -> 32
#         self.bn2 = nn.BatchNorm1d(32)

#         self.fc3 = nn.Linear(32, 1)                 # 32 -> 1

#         self.dropout = nn.Dropout(0.4)
#         self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)
#         self.silu = nn.SiLU()
#         self.relu = nn.ReLU()
#         self.sigmoid = nn.Sigmoid()
#         self.softmax = nn.Softmax()

#     def forward(self, x):
#         if x.dim() > 2:
#             x = x.squeeze(-1).squeeze(-1).squeeze(-1)
#             x = x.view(-1, x.size(-1))

#         x = self.relu(self.fc1(x))
#         x = self.dropout(x)
#         hidden = x
#         x = self.relu(self.fc2(x))
#         x = self.dropout(x)
#         x = self.sigmoid(self.fc3(x))
#         x = self.dropout(x)

#         return x.squeeze(-1)

    # def __init__(self, input_dim=2048, hidden_dim=1024):
    #     super(SequentialMILModel, self).__init__()
    #     print("Complex SequentialMILModel Initialized")

    #     #1024,512,256,128,64,32,1
        
    #     self.fc1 = nn.Linear(input_dim, hidden_dim) #2048 - 1024
    #     self.bn1 = nn.BatchNorm1d(hidden_dim)
        
    #     self.fc2 = nn.Linear(hidden_dim, 512) #1024 - 512
    #     self.bn2 = nn.BatchNorm1d(512)

    #     self.fc3 = nn.Linear(512, 256) # 512 -256
    #     self.bn3 = nn.BatchNorm1d(256)

    #     self.fc4 = nn.Linear(256, 128)
    #     self.bn4=nn.BatchNorm1d(128)

    #     self.fc5 = nn.Linear(128,64)
    #     self.bn5=nn.BatchNorm1d(64)

    #     self.fc6 =nn.Linear(64,32)

    #     self.fc7 = nn.Linear(32,1)
        
        
    #     self.dropout = nn.Dropout(0.4)
    #     self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)
    #     self.silu = nn.SiLU()
    #     self.relu = nn.ReLU()
    #     self.sigmoid = nn.Sigmoid()

    # def forward(self, x):
    #     if x.dim() > 2:
    #         x = x.squeeze(-1).squeeze(-1).squeeze(-1)
    #         x = x.view(-1, x.size(-1))

    #     x = self.fc1(x)
    #     x = self.relu(x)

    #     x = self.fc2(x)
    #     x = self.relu(x)
        
    #     x = self.fc3(x)
    #     x = self.relu(x)

    #     x = self.fc4(x)
    #     x = self.relu(x)

    #     x = self.fc5(x)
    #     x = self.relu(x)

    #     x = self.fc6(x)
    #     x = self.relu(x)

    #     x = self.fc7(x)
    #     x = self.sigmoid(x)

    #     return x.squeeze(-1)



--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\description
File type: 
Unnamed repository; edit this file 'description' to name the repository.


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\config
File type: 
[core]
	repositoryformatversion = 0
	filemode = false
	bare = false
	logallrefupdates = true
	symlinks = false
	ignorecase = true


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\HEAD
File type: 
ref: refs/heads/main


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\applypatch-msg.sample
File type: .sample
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to "applypatch-msg".

. git-sh-setup
commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
:


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\commit-msg.sample
File type: .sample
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by "git commit" with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to "commit-msg".

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"

# This example catches duplicate Signed-off-by lines.

test "" = "$(grep '^Signed-off-by: ' "$1" |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
	echo >&2 Duplicate Signed-off-by lines.
	exit 1
}


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\fsmonitor-watchman.sample
File type: .sample
#!/usr/bin/perl

use strict;
use warnings;
use IPC::Open2;

# An example hook script to integrate Watchman
# (https://facebook.github.io/watchman/) with git to speed up detecting
# new and modified files.
#
# The hook is passed a version (currently 2) and last update token
# formatted as a string and outputs to stdout a new update token and
# all files that have been modified since the update token. Paths must
# be relative to the root of the working tree and separated by a single NUL.
#
# To enable this hook, rename this file to "query-watchman" and set
# 'git config core.fsmonitor .git/hooks/query-watchman'
#
my ($version, $last_update_token) = @ARGV;

# Uncomment for debugging
# print STDERR "$0 $version $last_update_token\n";

# Check the hook interface version
if ($version ne 2) {
	die "Unsupported query-fsmonitor hook version '$version'.\n" .
	    "Falling back to scanning...\n";
}

my $git_work_tree = get_working_dir();

my $retry = 1;

my $json_pkg;
eval {
	require JSON::XS;
	$json_pkg = "JSON::XS";
	1;
} or do {
	require JSON::PP;
	$json_pkg = "JSON::PP";
};

launch_watchman();

sub launch_watchman {
	my $o = watchman_query();
	if (is_work_tree_watched($o)) {
		output_result($o->{clock}, @{$o->{files}});
	}
}

sub output_result {
	my ($clockid, @files) = @_;

	# Uncomment for debugging watchman output
	# open (my $fh, ">", ".git/watchman-output.out");
	# binmode $fh, ":utf8";
	# print $fh "$clockid\n@files\n";
	# close $fh;

	binmode STDOUT, ":utf8";
	print $clockid;
	print "\0";
	local $, = "\0";
	print @files;
}

sub watchman_clock {
	my $response = qx/watchman clock "$git_work_tree"/;
	die "Failed to get clock id on '$git_work_tree'.\n" .
		"Falling back to scanning...\n" if $? != 0;

	return $json_pkg->new->utf8->decode($response);
}

sub watchman_query {
	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
	or die "open2() failed: $!\n" .
	"Falling back to scanning...\n";

	# In the query expression below we're asking for names of files that
	# changed since $last_update_token but not from the .git folder.
	#
	# To accomplish this, we're using the "since" generator to use the
	# recency index to select candidate nodes and "fields" to limit the
	# output to file names only. Then we're using the "expression" term to
	# further constrain the results.
	my $last_update_line = "";
	if (substr($last_update_token, 0, 1) eq "c") {
		$last_update_token = "\"$last_update_token\"";
		$last_update_line = qq[\n"since": $last_update_token,];
	}
	my $query = <<"	END";
		["query", "$git_work_tree", {$last_update_line
			"fields": ["name"],
			"expression": ["not", ["dirname", ".git"]]
		}]
	END

	# Uncomment for debugging the watchman query
	# open (my $fh, ">", ".git/watchman-query.json");
	# print $fh $query;
	# close $fh;

	print CHLD_IN $query;
	close CHLD_IN;
	my $response = do {local $/; <CHLD_OUT>};

	# Uncomment for debugging the watch response
	# open ($fh, ">", ".git/watchman-response.json");
	# print $fh $response;
	# close $fh;

	die "Watchman: command returned no output.\n" .
	"Falling back to scanning...\n" if $response eq "";
	die "Watchman: command returned invalid output: $response\n" .
	"Falling back to scanning...\n" unless $response =~ /^\{/;

	return $json_pkg->new->utf8->decode($response);
}

sub is_work_tree_watched {
	my ($output) = @_;
	my $error = $output->{error};
	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
		$retry--;
		my $response = qx/watchman watch "$git_work_tree"/;
		die "Failed to make watchman watch '$git_work_tree'.\n" .
		    "Falling back to scanning...\n" if $? != 0;
		$output = $json_pkg->new->utf8->decode($response);
		$error = $output->{error};
		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		# Uncomment for debugging watchman output
		# open (my $fh, ">", ".git/watchman-output.out");
		# close $fh;

		# Watchman will always return all files on the first query so
		# return the fast "everything is dirty" flag to git and do the
		# Watchman query just to get it over with now so we won't pay
		# the cost in git to look up each individual file.
		my $o = watchman_clock();
		$error = $output->{error};

		die "Watchman: $error.\n" .
		"Falling back to scanning...\n" if $error;

		output_result($o->{clock}, ("/"));
		$last_update_token = $o->{clock};

		eval { launch_watchman() };
		return 0;
	}

	die "Watchman: $error.\n" .
	"Falling back to scanning...\n" if $error;

	return 1;
}

sub get_working_dir {
	my $working_dir;
	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
		$working_dir = Win32::GetCwd();
		$working_dir =~ tr/\\/\//;
	} else {
		require Cwd;
		$working_dir = Cwd::cwd();
	}

	return $working_dir;
}


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\post-update.sample
File type: .sample
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to "post-update".

exec git update-server-info


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\pre-applypatch.sample
File type: .sample
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-applypatch".

. git-sh-setup
precommit="$(git rev-parse --git-path hooks/pre-commit)"
test -x "$precommit" && exec "$precommit" ${1+"$@"}
:


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\pre-commit.sample
File type: .sample
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git commit" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-commit".

if git rev-parse --verify HEAD >/dev/null 2>&1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=$(git hash-object -t tree /dev/null)
fi

# If you want to allow non-ASCII filenames set this variable to true.
allownonascii=$(git config --type=bool hooks.allownonascii)

# Redirect output to stderr.
exec 1>&2

# Cross platform projects tend to avoid non-ASCII filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ "$allownonascii" != "true" ] &&
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test $(git diff-index --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
then
	cat <<\EOF
Error: Attempt to add a non-ASCII file name.

This can cause problems if you want to work with people on other platforms.

To be portable it is advisable to rename the file.

If you know what you are doing you can disable this check using:

  git config hooks.allownonascii true
EOF
	exit 1
fi

# If there are whitespace errors, print the offending file names and fail.
exec git diff-index --check --cached $against --


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\pre-merge-commit.sample
File type: .sample
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git merge" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message to
# stderr if it wants to stop the merge commit.
#
# To enable this hook, rename this file to "pre-merge-commit".

. git-sh-setup
test -x "$GIT_DIR/hooks/pre-commit" &&
        exec "$GIT_DIR/hooks/pre-commit"
:


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\pre-push.sample
File type: .sample
#!/bin/sh

# An example hook script to verify what is about to be pushed.  Called by "git
# push" after it has checked the remote status, but before anything has been
# pushed.  If this script exits with a non-zero status nothing will be pushed.
#
# This hook is called with the following parameters:
#
# $1 -- Name of the remote to which the push is being done
# $2 -- URL to which the push is being done
#
# If pushing without using a named remote those arguments will be equal.
#
# Information about the commits which are being pushed is supplied as lines to
# the standard input in the form:
#
#   <local ref> <local oid> <remote ref> <remote oid>
#
# This sample shows how to prevent push of commits where the log message starts
# with "WIP" (work in progress).

remote="$1"
url="$2"

zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')

while read local_ref local_oid remote_ref remote_oid
do
	if test "$local_oid" = "$zero"
	then
		# Handle delete
		:
	else
		if test "$remote_oid" = "$zero"
		then
			# New branch, examine all commits
			range="$local_oid"
		else
			# Update to existing branch, examine new commits
			range="$remote_oid..$local_oid"
		fi

		# Check for WIP commit
		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
		if test -n "$commit"
		then
			echo >&2 "Found WIP commit in $local_ref, not pushing"
			exit 1
		fi
	fi
done

exit 0


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\pre-rebase.sample
File type: .sample
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The "pre-rebase" hook is run just before "git rebase" starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch="$1"
if test "$#" = 2
then
	topic="refs/heads/$2"
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case "$topic" in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q "$topic" || {
	echo >&2 "No such branch $topic"
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
if test -z "$not_in_master"
then
	echo >&2 "$topic is fully merged to master; better remove it."
	exit 1 ;# we could allow it, but there is no point.
fi

# Is topic ever merged to next?  If so you should not be rebasing it.
only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
only_next_2=`git rev-list ^master           ${publish} | sort`
if test "$only_next_1" = "$only_next_2"
then
	not_in_topic=`git rev-list "^$topic" master`
	if test -z "$not_in_topic"
	then
		echo >&2 "$topic is already up to date with master"
		exit 1 ;# we could allow it, but there is no point.
	else
		exit 0
	fi
else
	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
	/usr/bin/perl -e '
		my $topic = $ARGV[0];
		my $msg = "* $topic has commits already merged to public branch:\n";
		my (%not_in_next) = map {
			/^([0-9a-f]+) /;
			($1 => 1);
		} split(/\n/, $ARGV[1]);
		for my $elem (map {
				/^([0-9a-f]+) (.*)$/;
				[$1 => $2];
			} split(/\n/, $ARGV[2])) {
			if (!exists $not_in_next{$elem->[0]}) {
				if ($msg) {
					print STDERR $msg;
					undef $msg;
				}
				print STDERR " $elem->[1]\n";
			}
		}
	' "$topic" "$not_in_next" "$not_in_master"
	exit 1
fi

<<\DOC_END

This sample hook safeguards topic branches that have been
published from being rewound.

The workflow assumed here is:

 * Once a topic branch forks from "master", "master" is never
   merged into it again (either directly or indirectly).

 * Once a topic branch is fully cooked and merged into "master",
   it is deleted.  If you need to build on top of it to correct
   earlier mistakes, a new topic branch is created by forking at
   the tip of the "master".  This is not strictly necessary, but
   it makes it easier to keep your history simple.

 * Whenever you need to test or publish your changes to topic
   branches, merge them into "next" branch.

The script, being an example, hardcodes the publish branch name
to be "next", but it is trivial to make it configurable via
$GIT_DIR/config mechanism.

With this workflow, you would want to know:

(1) ... if a topic branch has ever been merged to "next".  Young
    topic branches can have stupid mistakes you would rather
    clean up before publishing, and things that have not been
    merged into other branches can be easily rebased without
    affecting other people.  But once it is published, you would
    not want to rewind it.

(2) ... if a topic branch has been fully merged to "master".
    Then you can delete it.  More importantly, you should not
    build on top of it -- other people may already want to
    change things related to the topic as patches against your
    "master", so if you need further changes, it is better to
    fork the topic (perhaps with the same name) afresh from the
    tip of "master".

Let's look at this example:

		   o---o---o---o---o---o---o---o---o---o "next"
		  /       /           /           /
		 /   a---a---b A     /           /
		/   /               /           /
	       /   /   c---c---c---c B         /
	      /   /   /             \         /
	     /   /   /   b---b C     \       /
	    /   /   /   /             \     /
    ---o---o---o---o---o---o---o---o---o---o---o "master"


A, B and C are topic branches.

 * A has one fix since it was merged up to "next".

 * B has finished.  It has been fully merged up to "master" and "next",
   and is ready to be deleted.

 * C has not merged to "next" at all.

We would want to allow C to be rebased, refuse A, and encourage
B to be deleted.

To compute (1):

	git rev-list ^master ^topic next
	git rev-list ^master        next

	if these match, topic has not merged in next at all.

To compute (2):

	git rev-list master..topic

	if this is empty, it is fully merged to "master".

DOC_END


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\pre-receive.sample
File type: .sample
#!/bin/sh
#
# An example hook script to make use of push options.
# The example simply echoes all push options that start with 'echoback='
# and rejects all pushes when the "reject" push option is used.
#
# To enable this hook, rename this file to "pre-receive".

if test -n "$GIT_PUSH_OPTION_COUNT"
then
	i=0
	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
	do
		eval "value=\$GIT_PUSH_OPTION_$i"
		case "$value" in
		echoback=*)
			echo "echo from the pre-receive-hook: ${value#*=}" >&2
			;;
		reject)
			exit 1
		esac
		i=$((i + 1))
	done
fi


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\prepare-commit-msg.sample
File type: .sample
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by "git commit" with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to "prepare-commit-msg".

# This hook includes three examples. The first one removes the
# "# Please enter the commit message..." help message.
#
# The second includes the output of "git diff --name-status -r"
# into the message, just before the "git status" output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

COMMIT_MSG_FILE=$1
COMMIT_SOURCE=$2
SHA1=$3

/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"

# case "$COMMIT_SOURCE,$SHA1" in
#  ,|template,)
#    /usr/bin/perl -i.bak -pe '
#       print "\n" . `git diff --cached --name-status -r`
# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
#  *) ;;
# esac

# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
# if test -z "$COMMIT_SOURCE"
# then
#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
# fi


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\push-to-checkout.sample
File type: .sample
#!/bin/sh

# An example hook script to update a checked-out tree on a git push.
#
# This hook is invoked by git-receive-pack(1) when it reacts to git
# push and updates reference(s) in its repository, and when the push
# tries to update the branch that is currently checked out and the
# receive.denyCurrentBranch configuration variable is set to
# updateInstead.
#
# By default, such a push is refused if the working tree and the index
# of the remote repository has any difference from the currently
# checked out commit; when both the working tree and the index match
# the current commit, they are updated to match the newly pushed tip
# of the branch. This hook is to be used to override the default
# behaviour; however the code below reimplements the default behaviour
# as a starting point for convenient modification.
#
# The hook receives the commit with which the tip of the current
# branch is going to be updated:
commit=$1

# It can exit with a non-zero status to refuse the push (when it does
# so, it must not modify the index or the working tree).
die () {
	echo >&2 "$*"
	exit 1
}

# Or it can make any necessary changes to the working tree and to the
# index to bring them to the desired state when the tip of the current
# branch is updated to the new commit, and exit with a zero status.
#
# For example, the hook can simply run git read-tree -u -m HEAD "$1"
# in order to emulate git fetch that is run in the reverse direction
# with git push, as the two-tree form of git read-tree -u -m is
# essentially the same as git switch or git checkout that switches
# branches while keeping the local changes in the working tree that do
# not interfere with the difference between the branches.

# The below is a more-or-less exact translation to shell of the C code
# for the default behaviour for git's push-to-checkout hook defined in
# the push_to_deploy() function in builtin/receive-pack.c.
#
# Note that the hook will be executed from the repository directory,
# not from the working tree, so if you want to perform operations on
# the working tree, you will have to adapt your code accordingly, e.g.
# by adding "cd .." or using relative paths.

if ! git update-index -q --ignore-submodules --refresh
then
	die "Up-to-date check failed"
fi

if ! git diff-files --quiet --ignore-submodules --
then
	die "Working directory has unstaged changes"
fi

# This is a rough translation of:
#
#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
if git cat-file -e HEAD 2>/dev/null
then
	head=HEAD
else
	head=$(git hash-object -t tree --stdin </dev/null)
fi

if ! git diff-index --quiet --cached --ignore-submodules $head --
then
	die "Working directory has staged changes"
fi

if ! git read-tree -u -m "$commit"
then
	die "Could not update working tree to new HEAD"
fi


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\sendemail-validate.sample
File type: .sample
#!/bin/sh

# An example hook script to validate a patch (and/or patch series) before
# sending it via email.
#
# The hook should exit with non-zero status after issuing an appropriate
# message if it wants to prevent the email(s) from being sent.
#
# To enable this hook, rename this file to "sendemail-validate".
#
# By default, it will only check that the patch(es) can be applied on top of
# the default upstream branch without conflicts in a secondary worktree. After
# validation (successful or not) of the last patch of a series, the worktree
# will be deleted.
#
# The following config variables can be set to change the default remote and
# remote ref that are used to apply the patches against:
#
#   sendemail.validateRemote (default: origin)
#   sendemail.validateRemoteRef (default: HEAD)
#
# Replace the TODO placeholders with appropriate checks according to your
# needs.

validate_cover_letter () {
	file="$1"
	# TODO: Replace with appropriate checks (e.g. spell checking).
	true
}

validate_patch () {
	file="$1"
	# Ensure that the patch applies without conflicts.
	git am -3 "$file" || return
	# TODO: Replace with appropriate checks for this patch
	# (e.g. checkpatch.pl).
	true
}

validate_series () {
	# TODO: Replace with appropriate checks for the whole series
	# (e.g. quick build, coding style checks, etc.).
	true
}

# main -------------------------------------------------------------------------

if test "$GIT_SENDEMAIL_FILE_COUNTER" = 1
then
	remote=$(git config --default origin --get sendemail.validateRemote) &&
	ref=$(git config --default HEAD --get sendemail.validateRemoteRef) &&
	worktree=$(mktemp --tmpdir -d sendemail-validate.XXXXXXX) &&
	git worktree add -fd --checkout "$worktree" "refs/remotes/$remote/$ref" &&
	git config --replace-all sendemail.validateWorktree "$worktree"
else
	worktree=$(git config --get sendemail.validateWorktree)
fi || {
	echo "sendemail-validate: error: failed to prepare worktree" >&2
	exit 1
}

unset GIT_DIR GIT_WORK_TREE
cd "$worktree" &&

if grep -q "^diff --git " "$1"
then
	validate_patch "$1"
else
	validate_cover_letter "$1"
fi &&

if test "$GIT_SENDEMAIL_FILE_COUNTER" = "$GIT_SENDEMAIL_FILE_TOTAL"
then
	git config --unset-all sendemail.validateWorktree &&
	trap 'git worktree remove -ff "$worktree"' EXIT &&
	validate_series
fi


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\hooks\update.sample
File type: .sample
#!/bin/sh
#
# An example hook script to block unannotated tags from entering.
# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to "update".
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname="$1"
oldrev="$2"
newrev="$3"

# --- Safety check
if [ -z "$GIT_DIR" ]; then
	echo "Don't run this script from the command line." >&2
	echo " (if you want, you could supply GIT_DIR then run" >&2
	echo "  $0 <ref> <oldrev> <newrev>)" >&2
	exit 1
fi

if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
	exit 1
fi

# --- Config
allowunannotated=$(git config --type=bool hooks.allowunannotated)
allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
allowmodifytag=$(git config --type=bool hooks.allowmodifytag)

# check for no description
projectdesc=$(sed -e '1q' "$GIT_DIR/description")
case "$projectdesc" in
"Unnamed repository"* | "")
	echo "*** Project description file hasn't been set" >&2
	exit 1
	;;
esac

# --- Check types
# if $newrev is 0000...0000, it's a commit to delete a ref.
zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
if [ "$newrev" = "$zero" ]; then
	newrev_type=delete
else
	newrev_type=$(git cat-file -t $newrev)
fi

case "$refname","$newrev_type" in
	refs/tags/*,commit)
		# un-annotated tag
		short_refname=${refname##refs/tags/}
		if [ "$allowunannotated" != "true" ]; then
			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
			exit 1
		fi
		;;
	refs/tags/*,delete)
		# delete tag
		if [ "$allowdeletetag" != "true" ]; then
			echo "*** Deleting a tag is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/tags/*,tag)
		# annotated tag
		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
		then
			echo "*** Tag '$refname' already exists." >&2
			echo "*** Modifying a tag is not allowed in this repository." >&2
			exit 1
		fi
		;;
	refs/heads/*,commit)
		# branch
		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
			echo "*** Creating a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/heads/*,delete)
		# delete branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/remotes/*,commit)
		# tracking branch
		;;
	refs/remotes/*,delete)
		# delete tracking branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	*)
		# Anything else (is there anything else?)
		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
		exit 1
		;;
esac

# --- Finished
exit 0


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.git\info\exclude
File type: 
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~


--------------------------------------------------
File End
--------------------------------------------------


E:\MIL\Code\.kaggle\kaggle.json
File type: .json
{"username":"vikraman1001","key":"8b8392ddf85945839454af78a3f63941"}

--------------------------------------------------
File End
--------------------------------------------------
